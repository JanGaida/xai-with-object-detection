{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_StA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4eWRJXF_SiRX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9534ef89aef0432d928cb70eacea685d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_882e02eed8454dd3a8d8f8ca59b5d522",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4aa2b1892ec74c01aac7aba603025a54",
              "IPY_MODEL_52cd7321132c4eb8bd17692302514171",
              "IPY_MODEL_fb41fc9108484aceb7394f5442164cc9"
            ]
          }
        },
        "882e02eed8454dd3a8d8f8ca59b5d522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4aa2b1892ec74c01aac7aba603025a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_489c592e8d1246b6b4d83a516dca33f9",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52cd7321132c4eb8bd17692302514171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_65e886fdbcfd4fb5989b7f0d92df0dc6",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb41fc9108484aceb7394f5442164cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_c003dcc598dd46e4a82c94e105756437",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "489c592e8d1246b6b4d83a516dca33f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65e886fdbcfd4fb5989b7f0d92df0dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c003dcc598dd46e4a82c94e105756437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "235c8b5846a64f48993579c4b5dd208a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_96de7a8a054c432abb21ecb67465fc7a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e09014027ea943c68a53dbf20c23cec4",
              "IPY_MODEL_8bdab7b3fff14467b829f22dd9baf8f1",
              "IPY_MODEL_24b09623eec642cbb6192aff0cf189c2"
            ]
          }
        },
        "96de7a8a054c432abb21ecb67465fc7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e09014027ea943c68a53dbf20c23cec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_f904a38ac8264baab3fd3377057bebd1",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bdab7b3fff14467b829f22dd9baf8f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_8b8467cd25664730985130d302dc90b0",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24b09623eec642cbb6192aff0cf189c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_3996bd14ae6b4779beeb16d5b2089f47",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f904a38ac8264baab3fd3377057bebd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b8467cd25664730985130d302dc90b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3996bd14ae6b4779beeb16d5b2089f47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4168ddc11d5a4d74aaa82b74949bb94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc44213d07424337982af5169728369d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_278f10d473c347d4a716688f1d183bd3",
              "IPY_MODEL_709b82531f6d4da59b7f1dc09849a213",
              "IPY_MODEL_2c667b5b80b049cca555786b2b4c8a26"
            ]
          }
        },
        "fc44213d07424337982af5169728369d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "278f10d473c347d4a716688f1d183bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_278b81a3bae8446092c0406771397901",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "709b82531f6d4da59b7f1dc09849a213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_dc0b900dfffb489a8072edd146712265",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c667b5b80b049cca555786b2b4c8a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_8834943c384d4d9089096eec635f93a4",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "278b81a3bae8446092c0406771397901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc0b900dfffb489a8072edd146712265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8834943c384d4d9089096eec635f93a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f359e84ba0d04b0d8ff4829b2f1d1644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_578d36005e18438fb96723be4b8773d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7df81f9c1624469ae5a6cb3206d3211",
              "IPY_MODEL_e58fde614a6c42aa90a9dc5c94a90528",
              "IPY_MODEL_ab136543d06c4728bf2ca5c4b16b9926"
            ]
          }
        },
        "578d36005e18438fb96723be4b8773d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7df81f9c1624469ae5a6cb3206d3211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_88f3eb48878f4a2face4c2e2ecd59652",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e58fde614a6c42aa90a9dc5c94a90528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_b819428ff01949429b95ac1868e8a867",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab136543d06c4728bf2ca5c4b16b9926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "gif",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "325",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_68a10978a1604cb99b76317da186fc7f",
            "height": "325",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88f3eb48878f4a2face4c2e2ecd59652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b819428ff01949429b95ac1868e8a867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68a10978a1604cb99b76317da186fc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": "20px",
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYanvulZPcFu"
      },
      "source": [
        "# Preparations\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrzrcM3DDG0W"
      },
      "source": [
        "## *Requirements*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S09DtqmbEeP6"
      },
      "source": [
        "*In order to run this notebook we rely on specifc package-versions; here we will downgrade these nessecarry packages.*\n",
        "\n",
        "*After it's completion it is **required to restart** current running runtime-enviroment, this can be done by clicking 'Runtime' > 'Reset runtime' in the menubar.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIN5fxW5_zIf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK2UMyEqGaT-"
      },
      "source": [
        "## *Versioning*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC2dB9rOFTII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a4a8f2-9cb4-45c3-e3bb-1ac53b82aa7c"
      },
      "source": [
        "import torch\n",
        "import PIL\n",
        "import numpy as np\n",
        "\n",
        "print('Installed Packegesversions:\\n - PyTroch: {}\\n - Pillow:  {}\\n - Numpy:   {}'.format(torch.__version__, PIL.__version__, np.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed Packegesversions:\n",
            " - PyTroch: 1.10.0+cu111\n",
            " - Pillow:  7.1.2\n",
            " - Numpy:   1.19.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inwG1d3aP7R_"
      },
      "source": [
        "## *Imports*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhzzfR5n5VxX"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from torchvision import models\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import ipywidgets\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-0els8DQFm1"
      },
      "source": [
        "\n",
        "## *CNN-Visualization-Framework*\n",
        "\n",
        "* *Author: Utku Ozbulak*\n",
        "\n",
        "* *Year: 2019*\n",
        "\n",
        "* *Github: [utkuozbulak/pytorch-cnn-visualizations](https://github.com/utkuozbulak/pytorch-cnn-visualizations)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6Ef_3vRilq"
      },
      "source": [
        "We start by cloning this repository and checkout at a specifc commit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM26SQ4L6ZA-"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/utkuozbulak/pytorch-cnn-visualizations.git\n",
        "%cd /content/pytorch-cnn-visualizations\n",
        "!git checkout 66af4935d76cb0b597c33068026ca2a8e1c562dc\n",
        "%cd /content"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8avycvDRsw5"
      },
      "source": [
        "Now will make the delcared classes, variables and functions importable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-OEzfPk1oya"
      },
      "source": [
        "%%capture\n",
        "%cd /content/pytorch-cnn-visualizations\n",
        "with open('__init__.py', 'w') as f:\n",
        "  f.write('')\n",
        "%cd /content\n",
        "sys.path.append('/content/pytorch-cnn-visualizations/src')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5uO4qv5SFOV"
      },
      "source": [
        "And will finally import used classes, variables and functions into this python-enviorment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIHXM2SR_ZXI"
      },
      "source": [
        "from misc_functions import preprocess_image, save_image, apply_colormap_on_image"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtOENaWva019"
      },
      "source": [
        "## *ImageNet-Labels*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86JNY7Kya1u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5292b5-7d08-480c-c86b-996eaf3caa78"
      },
      "source": [
        "class ImageNetLabels:\n",
        "  \"\"\"\n",
        "  A Helper-class to resolve  imagenet-labels\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, p: str = '/content/imagenet_classes.txt'):\n",
        "    \"\"\"\n",
        "    Init\n",
        "\n",
        "    @param p: the path to the text-file to load labels from\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(p):\n",
        "      !wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "\n",
        "    self._labels = []\n",
        "    with open(p, 'r') as f:\n",
        "      for s in f.readlines():\n",
        "        self._labels.append(s.strip())\n",
        "\n",
        "  def decode_predictions(self, tensor, top_n) -> ([int], [str], [float]): \n",
        "    top_prob, top_catid = torch.topk(tensor, top_n)\n",
        "    class_ids = []\n",
        "    class_names = []\n",
        "    class_probabilities = []\n",
        "    for i in range(top_prob.size(0)):\n",
        "      id = top_catid[i].item()\n",
        "      class_ids.append(id)\n",
        "      class_names.append(self._labels[id])\n",
        "      class_probabilities.append(top_prob[i].item())\n",
        "    return (class_ids, class_names, class_probabilities)\n",
        "\n",
        "# the instance\n",
        "ImageNet_Labels = ImageNetLabels()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-12 17:40:10--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10472 (10K) [text/plain]\n",
            "Saving to: ‘imagenet_classes.txt’\n",
            "\n",
            "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-12 17:40:10 (68.7 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37w2AKgh4CuP"
      },
      "source": [
        "## *Progressbar*\n",
        "\n",
        "To show a HTML-progress bar when doing some intesive tasks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtMIBa2q4C-X"
      },
      "source": [
        "PROGRESS_BAR = None\n",
        "PROGRESS_BAR_VALUE = 0\n",
        "PROGRESS_BAR_MAX_VALUE = 100\n",
        "\n",
        "def progressbar(value, max_value):\n",
        "  \"\"\"\n",
        "  Creates a HTML-Progressbar\n",
        "\n",
        "  @param value: the value to set the prograssbar to\n",
        "  @param max_value: the max-value to set the prograssbar to\n",
        "  \"\"\"\n",
        "  return HTML(f'<progress value=\\'{value/max_value}\\' max=\\'{max}\\' style=\\'width: 100%\\'>\\'{value}%\\'</progress><label>Finished {value} / {max_value}</label>')   \n",
        "\n",
        "def display_progress_bar(init_value, max_value):\n",
        "  \"\"\"\n",
        "  Initializes and displays a HTML-Progressbar\n",
        "\n",
        "  @param init_value: the initial value to set the prograssbar to\n",
        "  @param max_value: the max-value to set the prograssbar to\n",
        "  \"\"\"\n",
        "  global PROGRESS_BAR\n",
        "  global PROGRESS_BAR_VALUE\n",
        "  global PROGRESS_BAR_MAX_VALUE\n",
        "  PROGRESS_BAR_VALUE = init_value\n",
        "  PROGRESS_BAR_MAX_VALUE = max_value\n",
        "  PROGRESS_BAR = display(progressbar(PROGRESS_BAR_VALUE, PROGRESS_BAR_MAX_VALUE), display_id=True)\n",
        "\n",
        "def update_progress_bar():\n",
        "  \"\"\"\n",
        "  Will incrementaly update the progress-value and update the displayed HTML-Progressbar\n",
        "  \"\"\"\n",
        "  global PROGRESS_BAR\n",
        "  global PROGRESS_BAR_VALUE\n",
        "  global PROGRESS_BAR_MAX_VALUE\n",
        "  PROGRESS_BAR_VALUE = PROGRESS_BAR_VALUE + 1\n",
        "  PROGRESS_BAR.update(progressbar(PROGRESS_BAR_VALUE, PROGRESS_BAR_MAX_VALUE))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oea1HGrRhOP"
      },
      "source": [
        "Some functionality to display multiple images:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA_4y53ikc-a"
      },
      "source": [
        "## *Matplotlib*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsNL2mktPBkd"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def extract_first_numbers(txt: str) -> str:\n",
        "  \"\"\"\n",
        "  Will extract all numbers out of given string\n",
        "\n",
        "  @param txt: the text to extract from\n",
        "\n",
        "  @return: the extracted numbers \n",
        "  \"\"\"\n",
        "  return ''.join([s for s in txt if s.isdigit()])\n",
        "\n",
        "def plot_images(image_paths: [str], rows: int, row_titles: [str], columns: int, column_titles: [str], figsize: (int, int), hide_column_title: bool=False):\n",
        "  \"\"\"\n",
        "  Will plot given images\n",
        "\n",
        "  @param image_paths: the paths of the images to display\n",
        "  @param rows: number of rows to use\n",
        "  @param columns: number of colums to use\n",
        "  @param figsize: the sizes used for the figure\n",
        "  @param tfunc: function called to transform a filename into a more meaningfull str\n",
        "  \"\"\"\n",
        "  # smaller figures so they can get rendered correctly in colab:\n",
        "  max_rows_per_figure = 10\n",
        "  if rows > max_rows_per_figure:\n",
        "    unscaled_figsize = (figsize[0], int(figsize[1]/rows))\n",
        "    sub_figure_count = math.ceil(rows/max_rows_per_figure)\n",
        "    split_image_paths = []\n",
        "    split_row_titles = []\n",
        "    i1, j1 = 0, max_rows_per_figure * columns\n",
        "    i2, j2 = 0, max_rows_per_figure\n",
        "    left_over = rows\n",
        "    for _ in range(0, sub_figure_count-1):\n",
        "      split_image_paths.append(image_paths[i1:j1])\n",
        "      split_row_titles.append(row_titles[i2:j2])\n",
        "      i1 = j1\n",
        "      j1 += (max_rows_per_figure * columns)\n",
        "      i2 = j2\n",
        "      j2 += max_rows_per_figure\n",
        "    split_image_paths.append(image_paths[i1:])\n",
        "    split_row_titles.append(row_titles[i2:])\n",
        "    \n",
        "    for i in range(0, sub_figure_count):\n",
        "      sub_rows = len(split_row_titles[i])\n",
        "      plot_images(split_image_paths[i], sub_rows, split_row_titles[i], columns, column_titles, (unscaled_figsize[0], unscaled_figsize[1] * sub_rows), bool(i))\n",
        "    return\n",
        "\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=rows, ncols=columns, figsize=figsize)\n",
        "  \n",
        "  # set column-title\n",
        "  if not hide_column_title:\n",
        "    if rows > 1:\n",
        "      for ax, ct in zip(axes[0], column_titles):\n",
        "        ax.set_title(ct, fontsize='16')\n",
        "    else:\n",
        "      axes[0].set_title(column_titles[0], fontsize='16')\n",
        "  \n",
        "  # set row-title\n",
        "  if rows > 1:\n",
        "    for ax, rt in zip(axes[:,0], row_titles):\n",
        "      ax.set_ylabel(rt, rotation=90, fontsize='16')\n",
        "  else:\n",
        "    axes[0].set_ylabel(row_titles[0], rotation=90, fontsize='16')\n",
        "\n",
        "  # image\n",
        "  for i, cell in enumerate(axes.flat):\n",
        "    cell.imshow(mpimg.imread(image_paths[i]))\n",
        "\n",
        "  # styling\n",
        "  #fig.text(0.5, 1.004, title, fontsize='20', horizontalalignment='center', verticalalignment='top')\n",
        "  fig.tight_layout()\n",
        "  fig.patch.set_facecolor('xkcd:white')\n",
        "  plt.show()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDApCGjhRhf2"
      },
      "source": [
        "## *Utilities*\n",
        "\n",
        "Other utility-functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUDXVgTBRlKA"
      },
      "source": [
        "def chunks(lst, n):\n",
        "  \"\"\" \n",
        "  Yield successive n-sized chunks from lst.\n",
        "\n",
        "  @param lst: the list to split\n",
        "  @param n: the chunks to split the given list into\n",
        "  \"\"\"\n",
        "  for i in range(0, len(lst), n):\n",
        "      yield lst[i:i + n]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypCiE1URkwsB"
      },
      "source": [
        "## Gif\n",
        "\n",
        "Used i.e. to transform images into a GIF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTFNuB3skw-G"
      },
      "source": [
        "def make_gif_from_images(image_paths: [str], description: [], export_dir: str, gif_filename: str, duration: int, loop: int, gif_filetype: str = '.gif', gif_fromat: str = 'GIF', margin: int = 3):\n",
        "  assert len(image_paths) > 1, 'Can not make a gif out of a single path.'\n",
        "  assert len(image_paths) == len(description), 'Length of images does not match the length of available names.'\n",
        "\n",
        "  if not gif_filetype[0] == '.':\n",
        "    gif_filetype = '.' + gif_filetype\n",
        "  if not export_dir[-1] == '/':\n",
        "    export_dir += '/'\n",
        "\n",
        "  # the images\n",
        "  images = [Image.open(p) for p in image_paths]\n",
        "  # draw the texts\n",
        "  for i, image in enumerate(images):\n",
        "    if type(description[i]) == int:\n",
        "      description[i] = f'conv_{description[i]}'\n",
        "    iw, ih = image.size\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    tw, th = draw.textsize(description[i])\n",
        "\n",
        "    x = margin\n",
        "    y = ih - th - margin\n",
        "    draw.rectangle((x,y,x+tw,y+th), fill='black')\n",
        "    draw.text((x,y), description[i])\n",
        "\n",
        "  # finaly \n",
        "  filename = f'{export_dir}{gif_filename}{gif_filetype}'\n",
        "  images[0].save(\n",
        "      fp=filename,\n",
        "      format=gif_fromat, append_images=images[1:],\n",
        "      save_all=True, duration=duration, loop=loop\n",
        "  )\n",
        "  [image.close() for image in images]\n",
        "\n",
        "  return filename\n",
        "\n",
        "def show_gifs(gif_paths: [str], format: str = 'gif', width: int = 325, height: int = 325, padding: int = 20):\n",
        "  images = []\n",
        "  for path in gif_paths:\n",
        "    images.append(ipywidgets.Image(value=open(path, 'rb').read(), format=format, width=width, height=height, layout=ipywidgets.Layout(padding=f'{padding}px')))\n",
        "  hbox = ipywidgets.HBox(images)\n",
        "  display(hbox)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eWRJXF_SiRX"
      },
      "source": [
        "# Image-Samples\n",
        "\n",
        "---\n",
        "\n",
        "Here we declare a helper-class for our sample-images, which manages indices and offered some useful access methods to oure sample data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx8sMPnwT80q"
      },
      "source": [
        "class ImageSamples:\n",
        "  \"\"\"\n",
        "  A helper-class for Image-Sampls\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, image_paths: [], targeted_class: [] = None, start_index: int = -1):\n",
        "    \"\"\"\n",
        "    Init\n",
        "\n",
        "    @param image_paths: list of paths to images\n",
        "    @param targeted_class: list of targeted classes, if None the most likely detected class will be targeted\n",
        "    @param start_index: internal inital index\n",
        "    \"\"\"\n",
        "    self.__samples = image_paths\n",
        "    assert len(self.__samples) >= 1, 'There must be at least one given sample'\n",
        "    if targeted_class is None:\n",
        "      self.__classes = [None] * len(self.__samples)\n",
        "    else:\n",
        "      self.__classes = targeted_class\n",
        "      assert len(self.__samples) == len(self.__classes), 'Targeted classes must be of same length as given samples'\n",
        "    self.__i = start_index\n",
        "  \n",
        "  def next(self) -> bool:\n",
        "    \"\"\"\n",
        "    Will incrementality increase the index internaly used.\n",
        "\n",
        "    @return: whether there is a element left\n",
        "    \"\"\"\n",
        "    i = 1 + self.__i\n",
        "    if i >= len(self.__samples) or i < 0:\n",
        "      return False\n",
        "    else:\n",
        "      self.__i = i\n",
        "      return True\n",
        "\n",
        "  def reset_index(self):\n",
        "    \"\"\"\n",
        "    Will reset the index, tho the next-function can be called again\n",
        "    \"\"\"\n",
        "    self.__i = -1\n",
        "  \n",
        "  def get_preprocessed_image(self, colorcodec: str = 'RGB') -> Image:\n",
        "    \"\"\"\n",
        "    Will return preprocess the current image for current index\n",
        "\n",
        "    @param colorcodec: the colorcodec used to transform the image\n",
        "\n",
        "    @return: the preprocessed image\n",
        "    \"\"\"\n",
        "    return preprocess_image(Image.open(self.__samples[self.__i]).convert(colorcodec), resize_im=True)\n",
        "\n",
        "  def get_original_image(self, colorcodec: str = 'RGB') -> Image:\n",
        "    \"\"\"\n",
        "    Will return the original image for current index\n",
        "\n",
        "    @param colorcodec: the colorcodec used to transform the image\n",
        "\n",
        "    @return: the original image\n",
        "    \"\"\"\n",
        "    return Image.open(self.__samples[self.__i]).convert(colorcodec)\n",
        "\n",
        "  def get_file_name(self) -> str:\n",
        "    \"\"\"\n",
        "    Will return the filename for current index\n",
        "\n",
        "    @return: the filename (without file-extension)\n",
        "    \"\"\"\n",
        "    return self.__samples[self.__i].split('/')[-1].split('.')[0]\n",
        "\n",
        "  def get_sample(self) -> str:\n",
        "    \"\"\"\n",
        "    Will return the set sample for current index\n",
        "\n",
        "    @return: the sample \n",
        "    \"\"\"\n",
        "    return self.__samples[self.__i]\n",
        "\n",
        "  def get_targeted_class(self) -> int:\n",
        "    \"\"\"\n",
        "    Will return targeted class for current index\n",
        "\n",
        "    @return: the number of targeted class\n",
        "    \"\"\"\n",
        "    return self.__classes[self.__i]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.__samples)\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbC_y2h6pUL7"
      },
      "source": [
        "def default_image_samples() -> ImageSamples:\n",
        "  \"\"\"\n",
        "  Will return a 'default' ImageSamples-object\n",
        "\n",
        "  @return: a ImageSamples-object with some predefined values\n",
        "  \"\"\"\n",
        "  return ImageSamples(image_paths = [\n",
        "    '/content/pytorch-cnn-visualizations/input_images/snake.jpg',   \n",
        "    '/content/pytorch-cnn-visualizations/input_images/cat_dog.png', \n",
        "  ])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visj8EjXTi3V"
      },
      "source": [
        "# XAI-Functionality\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9iUPGV6Pqul"
      },
      "source": [
        "## *CamExtractor*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytw_BV_MPrRJ"
      },
      "source": [
        "class CamExtractor():\n",
        "  \"\"\"\n",
        "  Extracts cam features from the model\n",
        "  \"\"\"\n",
        "  def __init__(self, model, target_layer):\n",
        "    self.model = model\n",
        "    self.target_layer = target_layer\n",
        "    self.gradients = None\n",
        "\n",
        "  def save_gradient(self, grad):\n",
        "    self.gradients = grad\n",
        "\n",
        "  def forward_pass_on_convolutions(self, x):\n",
        "    \"\"\"\n",
        "    Does a forward pass on convolutions, hooks the function at given layer\n",
        "\n",
        "    @param x: input-tensor\n",
        "\n",
        "    @return target-layer-output, output-tensor\n",
        "    \"\"\"\n",
        "    conv_output = None\n",
        "\n",
        "    # vgg\n",
        "    if type(self.model) == models.vgg.VGG:\n",
        "      for module_pos, module in self.model.features._modules.items():\n",
        "        x = module(x)  # Forward\n",
        "        if int(module_pos) == self.target_layer:\n",
        "          x.register_hook(self.save_gradient)\n",
        "          conv_output = x  # Save the convolution output on that layer\n",
        "\n",
        "    # resnet\n",
        "    elif type(self.model) == models.resnet.ResNet:\n",
        "      # header\n",
        "      x = self.model.conv1(x)\n",
        "      if self.target_layer == 'conv1':\n",
        "        x.register_hook(self.save_gradient)\n",
        "        conv_output = x\n",
        "      x = self.model.bn1(x)\n",
        "      x = self.model.relu(x)\n",
        "      x = self.model.maxpool(x)\n",
        "\n",
        "      # layers\n",
        "      for l, layer in enumerate([self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4]):\n",
        "        if self.target_layer.startswith(f'layer{l+1}_'):\n",
        "          for i, module in enumerate(layer):\n",
        "            identity = x\n",
        "\n",
        "            x = module.conv1(x)\n",
        "            if self.target_layer == f'layer{l+1}_{i}_conv1':\n",
        "              x.register_hook(self.save_gradient)\n",
        "              conv_output = x\n",
        "            x = module.bn1(x)\n",
        "            x = module.relu(x)\n",
        "\n",
        "            x = module.conv2(x)\n",
        "            if self.target_layer == f'layer{l+1}_{i}_conv2':\n",
        "              x.register_hook(self.save_gradient)\n",
        "              conv_output = x\n",
        "            x = module.bn2(x)\n",
        "            x = module.relu(x)\n",
        "\n",
        "            x = module.conv3(x)\n",
        "            if self.target_layer == f'layer{l+1}_{i}_conv3':\n",
        "              x.register_hook(self.save_gradient)\n",
        "              conv_output = x\n",
        "            x = module.bn3(x)\n",
        "\n",
        "            if module.downsample is not None:\n",
        "              if self.target_layer.startswith(f'layer{l+1}_{i}_downsample'):\n",
        "                for j, (_, i_module) in enumerate(module.downsample._modules.items()):\n",
        "                    identity = i_module(identity)\n",
        "                    if self.target_layer == f'layer{l+1}_{i}_downsample_{j}':\n",
        "                      identity.register_hook(self.save_gradient)\n",
        "                      conv_output = identity\n",
        "              else:\n",
        "                  identity = module.downsample(identity)\n",
        "\n",
        "            x += identity\n",
        "            x = module.relu(x)\n",
        "\n",
        "        else:\n",
        "          x = layer(x)\n",
        "\n",
        "      # tail\n",
        "      x = self.model.avgpool(x)\n",
        "    \n",
        "    else:\n",
        "      assert False, 'Unsupported model.'\n",
        "      \n",
        "    # output\n",
        "    return conv_output, x\n",
        "\n",
        "  def forward_pass(self, x):\n",
        "    \"\"\"\n",
        "    Does a full forward pass on the model\n",
        "\n",
        "    @param x: input-tensor\n",
        "\n",
        "    @return target-layer-output, output-tensor\n",
        "    \"\"\"\n",
        "    # Forward pass on the convolutions\n",
        "    conv_output, x = self.forward_pass_on_convolutions(x)\n",
        "    \n",
        "    # Forward pass on the classifier\n",
        "    if type(self.model) == models.vgg.VGG:\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.model.classifier(x)\n",
        "    elif type(self.model) == models.resnet.ResNet:\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = self.model.fc(x)\n",
        "    else:\n",
        "      assert False, 'Unsupported model.'\n",
        "    \n",
        "    return conv_output, x"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlOh_mtJhfk3"
      },
      "source": [
        "## *GradientCam*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKApcJxrTjLr"
      },
      "source": [
        "class GradientCam:\n",
        "  \"\"\"\n",
        "  A Helper-Class wrapping functions to build a gradient-cam\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, gradient_cam_export_path: str = 'grad_cam', cam_heatmap_file_desc: str = '_cam_heatmap', cam_on_image_heatmap_file_desc: str = '_cam_on_image', cam_grayscale_file_desc: str = '_cam_grayscale', save_file_type: str = '.png', filter_n_predicitons: int = 15, prediction_desc: str = 'predictions', save_prediction_file_type: str = '.txt'):\n",
        "    \"\"\"\n",
        "    Init\n",
        "\n",
        "    @param gradient_cam_export_path: the directory name/path to use when exporrting gradientcams\n",
        "    @param cam_heatmap_file_desc: description for a exported heatmap gradientcam\n",
        "    @param cam_on_image_heatmap_file_desc: description for a heatmap on the original image gradientcam\n",
        "    @param cam_grayscale_file_desc: description for a exported grayscale gradientcam\n",
        "    @param save_file_type: file-extension to use when exporting gradientcams\n",
        "    @param filter_n_predicitons: the amount of most-likely classes to filter out from the predictions\n",
        "    @param prediction_desc\n",
        "    @param save_prediction_file_type\n",
        "    \"\"\"\n",
        "    self.grad_cam = None\n",
        "    self.cam = None\n",
        "    self.model = None\n",
        "    self.predictions = None\n",
        "    self.gradient_cam_export_path = gradient_cam_export_path\n",
        "    self.cam_heatmap_file_desc = cam_heatmap_file_desc\n",
        "    self.cam_on_image_heatmap_file_desc = cam_on_image_heatmap_file_desc\n",
        "    self.cam_grayscale_file_desc = cam_grayscale_file_desc\n",
        "    if not save_file_type[0] == '.':\n",
        "      self.save_file_type = '.' + save_file_type\n",
        "    else:\n",
        "      self.save_file_type = save_file_type\n",
        "    self.filter_n_predicitons = filter_n_predicitons\n",
        "    self.prediction_desc = prediction_desc\n",
        "    self.save_prediction_file_type = save_prediction_file_type\n",
        "  \n",
        "  def run(self, model_wrapper, sample: ImageSamples, target_layer):\n",
        "    \"\"\"\n",
        "    Will calculate gradientcams\n",
        "\n",
        "    @param model_wrapper: the model-wrapper object to use\n",
        "    @param sample: the samples-object to use\n",
        "    @param target_layer: the targeted layer\n",
        "    \"\"\"\n",
        "    #print(f'Calculating GradientCam for Image \\'{sample.get_file_name()}\\' ...')\n",
        "    self.model = model_wrapper.model\n",
        "    self.model.eval()\n",
        "    self.extractor = CamExtractor(model_wrapper.model, target_layer)\n",
        "    self.cam = self._generate_cam(sample.get_preprocessed_image(), sample.get_targeted_class())\n",
        "\n",
        "  def _generate_cam(self, input_image, target_class=None):\n",
        "    \"\"\"\n",
        "    Modified version from GradCam.generate_cam\n",
        "     -> see https://github.com/utkuozbulak/pytorch-cnn-visualizations/blob/master/src/gradcam.py\n",
        "    \"\"\"\n",
        "    # Full forward pass\n",
        "    # conv_output is the output of convolutions at specified layer\n",
        "    # model_output is the final output of the model (1, 1000)\n",
        "    conv_output, model_output = self.extractor.forward_pass(input_image)\n",
        "    model_data_output = model_output.data.numpy()\n",
        "    # detrmine target_class\n",
        "    if target_class is None:\n",
        "      target_class = np.argmax(model_data_output)\n",
        "    self.predictions = ImageNet_Labels.decode_predictions(softmax(model_output[0], dim=0), self.filter_n_predicitons)\n",
        "    # Target for backprop\n",
        "    one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "    one_hot_output[0][target_class] = 1\n",
        "\n",
        "    # Zero grads\n",
        "    if type(self.model) == models.vgg.VGG:\n",
        "      self.model.features.zero_grad()\n",
        "      self.model.classifier.zero_grad()\n",
        "    elif type(self.model) == models.resnet.ResNet:\n",
        "      #[v.zero_grad() for k,v in self.model._modules.items()]\n",
        "      self.model.fc.zero_grad()\n",
        "\n",
        "    # Backward pass with specified target\n",
        "    model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "    # Get hooked gradients\n",
        "    guided_gradients = self.extractor.gradients.data.numpy()[0]\n",
        "    # Get convolution outputs\n",
        "    target = conv_output.data.numpy()[0]\n",
        "    # Get weights from gradients\n",
        "    weights = np.mean(guided_gradients, axis=(1, 2))  # Take averages for each gradient\n",
        "    # Create empty numpy array for cam\n",
        "    cam = np.ones(target.shape[1:], dtype=np.float32)\n",
        "    # Multiply each weight with its conv output and then, sum\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * target[i, :, :]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
        "    cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
        "    cam = np.uint8(Image.fromarray(cam).resize((input_image.shape[2], input_image.shape[3]), Image.ANTIALIAS))/255\n",
        "    return cam\n",
        "    \n",
        "  def __generate_export_dir_path(self, model_export_path) -> str:\n",
        "    \"\"\"\n",
        "    Will generate a full export-path to use when exporting gradientcams\n",
        "\n",
        "    @param model_export_path: the export-path declared in the model-wrapper\n",
        "    \"\"\"\n",
        "    export_dir = model_export_path\n",
        "    if not export_dir[-1] == '/':\n",
        "      export_dir += '/'\n",
        "    if self.gradient_cam_export_path[0] == '/':\n",
        "      self.gradient_cam_export_path = self.gradient_cam_export_path[1:]\n",
        "    export_dir += self.gradient_cam_export_path\n",
        "    return export_dir\n",
        "\n",
        "  def save_class_activation_images(self, model_wrapper, sample: ImageSamples, file_start: str = '', colormap_name: str = 'hsv'):\n",
        "    \"\"\"\n",
        "    Will export the gradientcams calculated\n",
        "\n",
        "    @param model_wrapper: the model model_wrapper used\n",
        "    @param sample: the samples used\n",
        "    @param file_start: a str added to the exported filename\n",
        "    @param colormap_name: the name of the colormap to use\n",
        "    \"\"\"\n",
        "    # Generate export path\n",
        "    export_dir = self.__generate_export_dir_path(model_wrapper.model_export_path)\n",
        "\n",
        "    # Create export-directory\n",
        "    if not os.path.exists(export_dir):\n",
        "        os.makedirs(export_dir)\n",
        "\n",
        "    # Grayscale activation map\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(samples.get_original_image(), self.cam, colormap_name)\n",
        "\n",
        "    # Save images\n",
        "    file_name = samples.get_file_name()    \n",
        "    save_image(heatmap, os.path.join(export_dir, file_start + file_name + self.cam_heatmap_file_desc + self.save_file_type))\n",
        "    save_image(heatmap_on_image, os.path.join(export_dir, file_start + file_name + self.cam_on_image_heatmap_file_desc + self.save_file_type))\n",
        "    save_image(self.cam, os.path.join(export_dir, file_start + file_name + self.cam_grayscale_file_desc + self.save_file_type))\n",
        "    #print(f'Saved GradientCam Image \\'{file_start+file_name}_*\\' to \\'{export_dir}\\' ...')\n",
        "\n",
        "    # Save predictions\n",
        "    with open(os.path.join(export_dir, file_start + file_name + self.prediction_desc + self.save_prediction_file_type), 'w') as f:\n",
        "      for id, cls, pred in zip(self.predictions[0], self.predictions[1], self.predictions[2]):\n",
        "        f.write(f'{str(id)},{str(cls)},{str(pred)}\\n')\n",
        "\n",
        "  def __get_prediction_info(self, prediction_path: str) -> str:\n",
        "    pred_info = []\n",
        "    with open(prediction_path, 'r') as f:\n",
        "      for line in f:\n",
        "        sl = line.rstrip().split(',')\n",
        "        pred_info.append(f'  {(sl[0]):>4s}: {sl[1]:30} with a Probability of {(float(sl[2]) * 100):6.2f} %')\n",
        "    return '\\n'.join(pred_info)\n",
        "\n",
        "  def show_class_activation_images(self, model_wrapper, sample: ImageSamples, images_per_row: int = 3, figsize: (int, int) = (200, 200)):\n",
        "    \"\"\"\n",
        "    Will display the calculated and exported images with matplot\n",
        "\n",
        "    @param model_wrapper: the model model_wrapper used\n",
        "    @param sample: the samples used\n",
        "    @param images_per_row: amount of images to display per row\n",
        "    @param figsize: the figure-sizes to use\n",
        "    \"\"\"\n",
        "    # Generate export path\n",
        "    export_dir = self.__generate_export_dir_path(model_wrapper.model_export_path)\n",
        "    file_names = []\n",
        "    samples.reset_index()\n",
        "    while samples.next():\n",
        "      file_names.append(samples.get_file_name())\n",
        "  \n",
        "    image_paths = []\n",
        "    prediction_paths = []\n",
        "    for file_name in file_names:\n",
        "      impaths = glob(f'{export_dir}/*{file_name}*{self.save_file_type}')\n",
        "      impaths.sort()\n",
        "      image_paths.append(impaths)\n",
        "      predpaths = glob(f'{export_dir}/*{file_name}*{self.save_prediction_file_type}')\n",
        "      predpaths.sort()\n",
        "      prediction_paths.append(predpaths)\n",
        "    \n",
        "    assert len(image_paths) == len(prediction_paths), 'Cant find image-paths with matching prediction files'\n",
        "\n",
        "    # column titles\n",
        "    column_titles = []\n",
        "    for path in image_paths[0:3]:\n",
        "      if self.cam_heatmap_file_desc in path:\n",
        "        column_titles.append('CAM (Heatmap)')\n",
        "      elif self.cam_on_image_heatmap_file_desc in path:\n",
        "        column_titles.append('CAM (Heatmap) on Image')\n",
        "      elif self.cam_grayscale_file_desc in path:\n",
        "        column_titles.append('CAM (Grayscale)')\n",
        "      else:\n",
        "        column_titles.append('')\n",
        "      \n",
        "    # row titles\n",
        "    row_titles = model_wrapper.get_targeted_layers()\n",
        "\n",
        "    for i, paths in enumerate(image_paths):\n",
        "      rows = math.ceil(len(paths)/images_per_row)\n",
        "      print(f'Image: {file_names[i]}\\nPredictions:\\n{self.__get_prediction_info(prediction_paths[i][0])}\\n')\n",
        "      plot_images(paths, rows, row_titles, images_per_row, column_titles, (figsize[0], figsize[1] * rows))\n",
        "      print('\\n')\n",
        "\n",
        "  def save_animated_gif(self, model_wrapper, sample: ImageSamples, duration: int = 1000, loop: int = 0):\n",
        "    # Generate export path\n",
        "    export_dir = self.__generate_export_dir_path(model_wrapper.model_export_path)\n",
        "    file_names = []\n",
        "    samples.reset_index()\n",
        "    while samples.next():\n",
        "      file_names.append(samples.get_file_name())\n",
        "\n",
        "    image_paths = []\n",
        "\n",
        "    for file_name in file_names:\n",
        "      _image_paths = glob(f'{export_dir}/*{file_name}*{self.save_file_type}')\n",
        "      _image_paths.sort()\n",
        "      image_paths.append(_image_paths)\n",
        "\n",
        "    out = []\n",
        "    for i, file_specific_image_paths in enumerate(image_paths):\n",
        "      heatmap = []\n",
        "      heatmap_on_image = []\n",
        "      grayscale = []\n",
        "      tmp = []\n",
        "      for image_path in file_specific_image_paths:\n",
        "        if self.cam_heatmap_file_desc in image_path:\n",
        "          heatmap.append(image_path)\n",
        "        elif self.cam_on_image_heatmap_file_desc in image_path:\n",
        "          heatmap_on_image.append(image_path)\n",
        "        elif self.cam_grayscale_file_desc in image_path:\n",
        "          grayscale.append(image_path)\n",
        "\n",
        "      tmp.append(make_gif_from_images(grayscale, model_wrapper.get_targeted_layers(), export_dir, f'_{file_names[i]}{self.cam_grayscale_file_desc}', duration, loop))\n",
        "      tmp.append(make_gif_from_images(heatmap, model_wrapper.get_targeted_layers(), export_dir, f'_{file_names[i]}{self.cam_heatmap_file_desc}', duration, loop))\n",
        "      tmp.append(make_gif_from_images(heatmap_on_image, model_wrapper.get_targeted_layers(), export_dir, f'_{file_names[i]}{self.cam_on_image_heatmap_file_desc}', duration, loop))\n",
        "      out.append(tmp)\n",
        "    \n",
        "    return out, file_names\n",
        "\n",
        "  def show_animated_gifs(self, model_wrapper, sample: ImageSamples, gif_paths: [[str]], filenames: [str]):\n",
        "    \n",
        "    # Generate export path\n",
        "    export_dir = self.__generate_export_dir_path(model_wrapper.model_export_path)\n",
        "    file_names = []\n",
        "    samples.reset_index()\n",
        "    while samples.next():\n",
        "      file_names.append(samples.get_file_name())\n",
        "  \n",
        "    prediction_paths = []\n",
        "    for file_name in file_names:\n",
        "      prediction_paths.append(glob(f'{export_dir}/*{file_name}*{self.save_prediction_file_type}')[0])\n",
        "\n",
        "    for filename, gif_path, prediction_path in zip(filenames, gif_paths, prediction_paths):\n",
        "      print('\\nPredictions:\\n{}'.format(self.__get_prediction_info(prediction_path)))\n",
        "      show_gifs(gif_path)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO1TG_y0SVYp"
      },
      "source": [
        "# Neuronal-Networks\n",
        "---\n",
        "\n",
        "Here we declare our Neuralnetwork-Wrappers, which is mainly responsible to give access to specified model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBQEJfASmMW"
      },
      "source": [
        "## *VGG19*\n",
        "\n",
        "The VGG-neural-network is a often used backbone for detection; it makes almost as less error as humans would do (et al. [Comparison of Backbones for Semantic\n",
        "Segmentation Network](https://iopscience.iop.org/article/10.1088/1742-6596/1544/1/012196/pdf)).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/0*E6BE6GDv-53smX0B.jpg\"/>\n",
        "\n",
        "<a src=\"https://koushik1102.medium.com/transfer-learning-with-vgg16-and-vgg19-the-simpler-way-ad4eec1e2997\">Source: Koushik kumar - Transfer learning with VGG16 and VGG19, the simpler way!</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqciakGBSgK-"
      },
      "source": [
        "class Wrapper_VGG19:\n",
        "  \"\"\"\n",
        "  A wrapper-class for the torch-model of vgg\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, pretrained: bool = True, model_export_path: str = '/content/results/vgg19'):\n",
        "    \"\"\"\n",
        "    Init\n",
        "\n",
        "    @param pretrained: whether to load pretrained weights\n",
        "    @param model_export_path: the export directory specific to the wrapped model\n",
        "    \"\"\"\n",
        "    self.pretrained = pretrained\n",
        "    self.model_export_path = model_export_path\n",
        "    self.model = None\n",
        "  \n",
        "  def build(self):\n",
        "    \"\"\"\n",
        "    Will build the model\n",
        "\n",
        "    @return self\n",
        "    \"\"\"\n",
        "    self.model = models.vgg19(pretrained=self.pretrained)\n",
        "    return self\n",
        "  \n",
        "  def summary(self):\n",
        "    \"\"\"\n",
        "    Will print layer-informationen about the model\n",
        "    \"\"\"\n",
        "    print(f'\\n{self.model}')\n",
        "\n",
        "  def get_targeted_layers(self) -> [int]:\n",
        "    \"\"\"\n",
        "    Returns a list of layer-indicies to consider for xai-methods\n",
        "\n",
        "    @return the requested list of layer-indicies\n",
        "    \"\"\"\n",
        "    #return [0,7,14,21,28,34]\n",
        "    return [0,2,5,7,10,12,14,16,19,21,23,25,28,30,32,34]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NfAZmVJXSu"
      },
      "source": [
        "## *ResNet152*\n",
        "\n",
        "The ResNet-neural-network is a powerfull and often used backbone for detection; it makes less error than a humans would do (et al. [Comparison of Backbones for Semantic\n",
        "Segmentation Network](https://iopscience.iop.org/article/10.1088/1742-6596/1544/1/012196/pdf)).\n",
        "\n",
        "<img src=\"https://www.mdpi.com/applsci/applsci-10-02528/article_deploy/html/images/applsci-10-02528-g002.png\"/>\n",
        "\n",
        "<a src=\"https://www.mdpi.com/2076-3417/10/7/2528/htm#fig_body_display_applsci-10-02528-f002\">Source: Region-Based CNN Method with Deformable Modules for Visually Classifying Concrete Cracks</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Ob1kVeJehH"
      },
      "source": [
        "class Wrapper_ResNet152:\n",
        "  \"\"\"\n",
        "  A wrapper-class for the torch-model of resnet\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, pretrained: bool = True, model_export_path: str = '/content/results/resnet152'):\n",
        "    \"\"\"\n",
        "    Init\n",
        "\n",
        "    @param pretrained: whether to load pretrained weights\n",
        "    @param model_export_path: the export directory specific to the wrapped model\n",
        "    \"\"\"\n",
        "    self.pretrained = pretrained\n",
        "    self.model_export_path = model_export_path\n",
        "    self.model = None\n",
        "\n",
        "  def build(self):\n",
        "    \"\"\"\n",
        "    Will build the model\n",
        "\n",
        "    @return self\n",
        "    \"\"\"\n",
        "    self.model = models.resnet152(pretrained=self.pretrained)\n",
        "    return self\n",
        "  \n",
        "  def summary(self):\n",
        "    \"\"\"\n",
        "    Will print layer-informationen about the model\n",
        "    \"\"\"\n",
        "    print(f'\\n{self.model}')\n",
        "\n",
        "  def get_targeted_layers(self) -> [str]:\n",
        "    \"\"\"\n",
        "    Returns a list of layer-indicies to consider for xai-methods\n",
        "\n",
        "    @return the requested list of layer-indicies\n",
        "    \"\"\"\n",
        "    #return ['conv1', 'layer1_0_conv1', 'layer1_2_conv1', 'layer2_0_conv1',  'layer2_2_conv1', 'layer2_4_conv1', 'layer2_7_conv1', 'layer3_0_conv1', 'layer3_5_conv1', 'layer3_10_conv1', 'layer3_15_conv1', 'layer3_20_conv1', 'layer3_25_conv1', 'layer3_30_conv1', 'layer3_35_conv1', 'layer4_0_conv1', 'layer4_2_conv1']\n",
        "    return ['conv1',\n",
        "      # layer1\n",
        "      'layer1_0_conv1', 'layer1_0_conv2', 'layer1_0_conv3', 'layer1_0_downsample_0',\n",
        "      'layer1_1_conv1', 'layer1_1_conv2', 'layer1_1_conv3',\n",
        "      'layer1_2_conv1', 'layer1_2_conv2', 'layer1_2_conv3',\n",
        "      # layer2\n",
        "      'layer2_0_conv1', 'layer2_0_conv2', 'layer2_0_conv3', 'layer2_0_downsample_0',\n",
        "      'layer2_1_conv1', 'layer2_1_conv2', 'layer2_1_conv3',\n",
        "      'layer2_2_conv1', 'layer2_2_conv2', 'layer2_2_conv3',\n",
        "      'layer2_3_conv1', 'layer2_3_conv2', 'layer2_3_conv3',\n",
        "      'layer2_4_conv1', 'layer2_4_conv2', 'layer2_4_conv3',\n",
        "      'layer2_5_conv1', 'layer2_5_conv2', 'layer2_5_conv3',\n",
        "      'layer2_6_conv1', 'layer2_6_conv2', 'layer2_6_conv3',\n",
        "      'layer2_7_conv1', 'layer2_7_conv2', 'layer2_7_conv3',\n",
        "      # layer3\n",
        "      'layer3_0_conv1', 'layer3_0_conv2', 'layer3_0_conv3', 'layer3_0_downsample_0',\n",
        "      'layer3_1_conv1', 'layer3_1_conv2', 'layer3_1_conv3',\n",
        "      'layer3_2_conv1', 'layer3_2_conv2', 'layer3_2_conv3',\n",
        "      'layer3_3_conv1', 'layer3_3_conv2', 'layer3_3_conv3',\n",
        "      'layer3_4_conv1', 'layer3_4_conv2', 'layer3_4_conv3',\n",
        "      'layer3_5_conv1', 'layer3_5_conv2', 'layer3_5_conv3',\n",
        "      'layer3_6_conv1', 'layer3_6_conv2', 'layer3_6_conv3',\n",
        "      'layer3_7_conv1', 'layer3_7_conv2', 'layer3_7_conv3',\n",
        "      'layer3_8_conv1', 'layer3_8_conv2', 'layer3_8_conv3',\n",
        "      'layer3_9_conv1', 'layer3_9_conv2', 'layer3_9_conv3',\n",
        "      'layer3_10_conv1', 'layer3_10_conv2', 'layer3_10_conv3',\n",
        "      'layer3_11_conv1', 'layer3_11_conv2', 'layer3_11_conv3',\n",
        "      'layer3_12_conv1', 'layer3_12_conv2', 'layer3_12_conv3',\n",
        "      'layer3_13_conv1', 'layer3_13_conv2', 'layer3_13_conv3',\n",
        "      'layer3_14_conv1', 'layer3_14_conv2', 'layer3_14_conv3',\n",
        "      'layer3_15_conv1', 'layer3_15_conv2', 'layer3_15_conv3',\n",
        "      'layer3_16_conv1', 'layer3_16_conv2', 'layer3_16_conv3',\n",
        "      'layer3_17_conv1', 'layer3_17_conv2', 'layer3_17_conv3',\n",
        "      'layer3_18_conv1', 'layer3_18_conv2', 'layer3_18_conv3',\n",
        "      'layer3_19_conv1', 'layer3_19_conv2', 'layer3_19_conv3',\n",
        "      'layer3_20_conv1', 'layer3_20_conv2', 'layer3_20_conv3',\n",
        "      'layer3_21_conv1', 'layer3_21_conv2', 'layer3_21_conv3',\n",
        "      'layer3_22_conv1', 'layer3_22_conv2', 'layer3_22_conv3',\n",
        "      'layer3_23_conv1', 'layer3_23_conv2', 'layer3_23_conv3',\n",
        "      'layer3_24_conv1', 'layer3_24_conv2', 'layer3_24_conv3',\n",
        "      'layer3_25_conv1', 'layer3_25_conv2', 'layer3_25_conv3',\n",
        "      'layer3_26_conv1', 'layer3_26_conv2', 'layer3_26_conv3',\n",
        "      'layer3_27_conv1', 'layer3_27_conv2', 'layer3_27_conv3',\n",
        "      'layer3_28_conv1', 'layer3_28_conv2', 'layer3_28_conv3',\n",
        "      'layer3_29_conv1', 'layer3_29_conv2', 'layer3_29_conv3',\n",
        "      'layer3_30_conv1', 'layer3_30_conv2', 'layer3_30_conv3',\n",
        "      'layer3_31_conv1', 'layer3_31_conv2', 'layer3_31_conv3',\n",
        "      'layer3_32_conv1', 'layer3_32_conv2', 'layer3_32_conv3',\n",
        "      'layer3_33_conv1', 'layer3_33_conv2', 'layer3_33_conv3',\n",
        "      'layer3_34_conv1', 'layer3_34_conv2', 'layer3_34_conv3',\n",
        "      'layer3_35_conv1', 'layer3_35_conv2', 'layer3_35_conv3',\n",
        "      # layer4\n",
        "      'layer4_0_conv1', 'layer4_0_conv2', 'layer4_0_conv3', 'layer4_0_downsample_0',\n",
        "      'layer4_1_conv1', 'layer4_1_conv2', 'layer4_1_conv3',\n",
        "      'layer4_2_conv1', 'layer4_2_conv2', 'layer4_2_conv3']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLjbyor3bGdC"
      },
      "source": [
        "# GradientCam\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CrXt4ibbFbd"
      },
      "source": [
        "To view the result's of the Gradient am, we start with initializing the Samples-Objekt and the GradientCam-Objekt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATTEg4ptbFsm"
      },
      "source": [
        "samples = default_image_samples()\n",
        "gradcam = GradientCam()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVUuLY9IhImm"
      },
      "source": [
        "## *VGG19*\n",
        "\n",
        "Next up we will initialize the neural Networks and run the GradientCam with them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrBxS5kijIC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7376c843-9c72-4314-f64b-1189ed45593b"
      },
      "source": [
        "vgg19 = Wrapper_VGG19(pretrained=True).build()\n",
        "vgg19.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7JwPrvwMiMs"
      },
      "source": [
        "Now we can run the gradcam with specified parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlSpg1gwqxUZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ed18bce3-b0fd-437d-e27d-7ac8d0316ace"
      },
      "source": [
        "# show displaybar\n",
        "display_progress_bar(0, len(vgg19.get_targeted_layers()) * len(samples))\n",
        "\n",
        "for targeted_layer in vgg19.get_targeted_layers():\n",
        "  # reset samples\n",
        "  samples.reset_index()\n",
        "  # loop all samples:\n",
        "  while samples.next():\n",
        "    # calculate\n",
        "    gradcam.run(vgg19, samples, targeted_layer)\n",
        "    gradcam.save_class_activation_images(vgg19, samples, file_start=f'{targeted_layer:02d}_', colormap_name='hsv')\n",
        "    # update progress\n",
        "    update_progress_bar()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<progress value='1.0' max='<built-in function max>' style='width: 100%'>'32%'</progress><label>Finished 32 / 32</label>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqGb4ldOMoW_"
      },
      "source": [
        "Let's have a look at generated images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6gHQQ6UNLva",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9534ef89aef0432d928cb70eacea685d",
            "882e02eed8454dd3a8d8f8ca59b5d522",
            "4aa2b1892ec74c01aac7aba603025a54",
            "52cd7321132c4eb8bd17692302514171",
            "fb41fc9108484aceb7394f5442164cc9",
            "489c592e8d1246b6b4d83a516dca33f9",
            "65e886fdbcfd4fb5989b7f0d92df0dc6",
            "c003dcc598dd46e4a82c94e105756437",
            "235c8b5846a64f48993579c4b5dd208a",
            "96de7a8a054c432abb21ecb67465fc7a",
            "e09014027ea943c68a53dbf20c23cec4",
            "8bdab7b3fff14467b829f22dd9baf8f1",
            "24b09623eec642cbb6192aff0cf189c2",
            "f904a38ac8264baab3fd3377057bebd1",
            "8b8467cd25664730985130d302dc90b0",
            "3996bd14ae6b4779beeb16d5b2089f47"
          ]
        },
        "outputId": "c09da802-e173-4e1f-c141-bc08fe485123"
      },
      "source": [
        "# Rendering all images may cause problems with jupyternotebook; uncomment at your own risk\n",
        "#gradcam.show_class_activation_images(vgg19, samples, figsize=(20, 6))\n",
        "gradcam.show_animated_gifs(vgg19, samples, *gradcam.save_animated_gif(vgg19, samples, duration=1100))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions:\n",
            "    56: king snake                     with a Probability of  60.62 %\n",
            "    60: night snake                    with a Probability of  30.63 %\n",
            "    68: sidewinder                     with a Probability of   3.38 %\n",
            "    54: hognose snake                  with a Probability of   1.52 %\n",
            "    66: horned viper                   with a Probability of   1.51 %\n",
            "    53: ringneck snake                 with a Probability of   0.53 %\n",
            "    58: water snake                    with a Probability of   0.44 %\n",
            "    67: diamondback                    with a Probability of   0.32 %\n",
            "    62: rock python                    with a Probability of   0.28 %\n",
            "    65: sea snake                      with a Probability of   0.26 %\n",
            "    52: thunder snake                  with a Probability of   0.20 %\n",
            "    61: boa constrictor                with a Probability of   0.18 %\n",
            "    63: Indian cobra                   with a Probability of   0.04 %\n",
            "    38: banded gecko                   with a Probability of   0.03 %\n",
            "    59: vine snake                     with a Probability of   0.02 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9534ef89aef0432d928cb70eacea685d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(Image(value=b'GIF89a\\xe0\\x00\\xe0\\x00\\x85\\x00\\x00\\x00\\x00\\x003\\x00\\x00\\x003\\x0033\\x00\\x00\\x0033\\…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions:\n",
            "   243: bull mastiff                   with a Probability of  46.06 %\n",
            "   245: French bulldog                 with a Probability of  19.10 %\n",
            "   254: pug                            with a Probability of  18.12 %\n",
            "   242: boxer                          with a Probability of   6.92 %\n",
            "   281: tabby                          with a Probability of   0.98 %\n",
            "   180: American Staffordshire terrier with a Probability of   0.83 %\n",
            "   539: doormat                        with a Probability of   0.60 %\n",
            "   179: Staffordshire bullterrier      with a Probability of   0.45 %\n",
            "   811: space heater                   with a Probability of   0.44 %\n",
            "   282: tiger cat                      with a Probability of   0.39 %\n",
            "   453: bookcase                       with a Probability of   0.38 %\n",
            "   753: radiator                       with a Probability of   0.27 %\n",
            "   182: Border terrier                 with a Probability of   0.23 %\n",
            "   999: toilet tissue                  with a Probability of   0.22 %\n",
            "   534: dishwasher                     with a Probability of   0.22 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "235c8b5846a64f48993579c4b5dd208a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(Image(value=b'GIF89a\\xe0\\x00\\xe0\\x00\\x85\\x00\\x00\\x00\\x00\\x003\\x00\\x00\\x003\\x0033\\x00\\x00\\x0033\\…"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SNpuRSvMywb"
      },
      "source": [
        "Let's clean up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntIv3AmzMwcL"
      },
      "source": [
        "del vgg19"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4IreMWMtMs"
      },
      "source": [
        "## *ResNet152*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxTaQ-2tMdVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8d6753-8cbe-48c7-b757-acce84d251ad"
      },
      "source": [
        "resnet152 = Wrapper_ResNet152(pretrained=True).build()\n",
        "resnet152.summary()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (13): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (14): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (15): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (16): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (17): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (18): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (19): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (20): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (21): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (22): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (23): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (24): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (25): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (26): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (27): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (28): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (29): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (30): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (31): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (32): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (33): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (34): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (35): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XCRBbUVNn_C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e93b9327-30b9-4bd4-e8ea-624e57316f56"
      },
      "source": [
        "# show displaybar\n",
        "display_progress_bar(0, len(resnet152.get_targeted_layers()) * len(samples))\n",
        "\n",
        "for targeted_layer in resnet152.get_targeted_layers():\n",
        "  # reset samples\n",
        "  samples.reset_index()\n",
        "  # loop all samples:\n",
        "  while samples.next():\n",
        "    # calculate\n",
        "    gradcam.run(resnet152, samples, targeted_layer)\n",
        "    gradcam.save_class_activation_images(resnet152, samples, file_start=f'{targeted_layer}_', colormap_name='hsv')\n",
        "    # update progress\n",
        "    update_progress_bar()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<progress value='1.0' max='<built-in function max>' style='width: 100%'>'310%'</progress><label>Finished 310 / 310</label>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLGcFz1Tj189"
      },
      "source": [
        "Let's have a look at generated images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dulgGYFn7Qzv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4168ddc11d5a4d74aaa82b74949bb94e",
            "fc44213d07424337982af5169728369d",
            "278f10d473c347d4a716688f1d183bd3",
            "709b82531f6d4da59b7f1dc09849a213",
            "2c667b5b80b049cca555786b2b4c8a26",
            "278b81a3bae8446092c0406771397901",
            "dc0b900dfffb489a8072edd146712265",
            "8834943c384d4d9089096eec635f93a4",
            "f359e84ba0d04b0d8ff4829b2f1d1644",
            "578d36005e18438fb96723be4b8773d6",
            "e7df81f9c1624469ae5a6cb3206d3211",
            "e58fde614a6c42aa90a9dc5c94a90528",
            "ab136543d06c4728bf2ca5c4b16b9926",
            "88f3eb48878f4a2face4c2e2ecd59652",
            "b819428ff01949429b95ac1868e8a867",
            "68a10978a1604cb99b76317da186fc7f"
          ]
        },
        "outputId": "472ca2e0-97ec-44b0-b198-a9a9a8caadd7"
      },
      "source": [
        "# Rendering all images may cause problems with jupyternotebook; uncomment at your own risk\n",
        "#gradcam.show_class_activation_images(resnet152, samples, figsize=(20, 6))\n",
        "gradcam.show_animated_gifs(resnet152, samples, *gradcam.save_animated_gif(resnet152, samples, duration=1100))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions:\n",
            "    60: night snake                    with a Probability of  48.22 %\n",
            "    56: king snake                     with a Probability of  38.90 %\n",
            "    68: sidewinder                     with a Probability of   5.01 %\n",
            "    66: horned viper                   with a Probability of   1.46 %\n",
            "    53: ringneck snake                 with a Probability of   1.19 %\n",
            "    65: sea snake                      with a Probability of   1.01 %\n",
            "    58: water snake                    with a Probability of   0.84 %\n",
            "    54: hognose snake                  with a Probability of   0.63 %\n",
            "    63: Indian cobra                   with a Probability of   0.60 %\n",
            "    38: banded gecko                   with a Probability of   0.50 %\n",
            "    52: thunder snake                  with a Probability of   0.41 %\n",
            "    67: diamondback                    with a Probability of   0.37 %\n",
            "    62: rock python                    with a Probability of   0.32 %\n",
            "    59: vine snake                     with a Probability of   0.18 %\n",
            "    61: boa constrictor                with a Probability of   0.08 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4168ddc11d5a4d74aaa82b74949bb94e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(Image(value=b'GIF89a\\xe0\\x00\\xe0\\x00\\x85\\x00\\x00\\x00\\x00\\x003\\x00\\x00\\x003\\x0033\\x00\\x00\\x0033\\…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions:\n",
            "   243: bull mastiff                   with a Probability of  54.28 %\n",
            "   282: tiger cat                      with a Probability of  19.30 %\n",
            "   242: boxer                          with a Probability of  10.43 %\n",
            "   281: tabby                          with a Probability of   4.34 %\n",
            "   245: French bulldog                 with a Probability of   1.58 %\n",
            "   753: radiator                       with a Probability of   1.18 %\n",
            "   180: American Staffordshire terrier with a Probability of   0.82 %\n",
            "   811: space heater                   with a Probability of   0.82 %\n",
            "   292: tiger                          with a Probability of   0.79 %\n",
            "   285: Egyptian cat                   with a Probability of   0.77 %\n",
            "   543: dumbbell                       with a Probability of   0.52 %\n",
            "   539: doormat                        with a Probability of   0.43 %\n",
            "   882: vacuum                         with a Probability of   0.35 %\n",
            "   478: carton                         with a Probability of   0.26 %\n",
            "   179: Staffordshire bullterrier      with a Probability of   0.25 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f359e84ba0d04b0d8ff4829b2f1d1644",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(Image(value=b'GIF89a\\xe0\\x00\\xe0\\x00\\x84\\x00\\x00\\x00\\x00\\x003\\x00\\x0033\\x00\\x00\\x003333ff333ff3…"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeWE5oHMj4Mu"
      },
      "source": [
        "Let's clean up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPly9rhPj35w"
      },
      "source": [
        "del resnet152"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}